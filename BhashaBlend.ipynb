{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cP8khWeM4q8B",
        "outputId": "6f2240e9-953d-45ba-f712-2f6f836a4dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-z0qd2yhe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-z0qd2yhe\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=79e4d76b60b618a3cc0303b9551859c331f497db2b538402e0f0ff9a30d6d56a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v5cvvfiu/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,223 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,555 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,871 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,041 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,175 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,135 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,424 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,497 kB]\n",
            "Fetched 24.3 MB in 4s (5,875 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        " ## IMPORTANT DEPENDENCIES TO RUN FIRST\n",
        " !pip install git+https://github.com/openai/whisper.git\n",
        " !sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E1tvNA8gRJGI",
        "outputId": "f4fde341-01cb-4eee-b822-f6fd4d2a8d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.7.4)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gTTS\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JFmkLaOHRL35",
        "outputId": "5cc63c28-607e-42e1-d904-0b409bfcf302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.7.4)\n",
            "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "bGZ0gxcdfuRs",
        "outputId": "ead19bd2-b55f-4ea3-a6e4-ab0dc70b2b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.7.4)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.8.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.8.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=6045c90fc493100e5f5362a0d072bff014544a3c379c0a0599ddb405a82bce71\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.8.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "891888ba6e0d498eb72d4c44430e4a36",
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install googletrans==4.0.0-rc1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pUuqmzyekX-d",
        "outputId": "ab13dbf4-641c-49a5-d4f0-1296bbe2f060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.3.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.1.4)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.42.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.6.20)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.7.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m885.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=773b4be3c62a233c3e31298069da19a25c521de360a45272c87b86451d46f1a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score, sentence-transformers, bert-score\n",
            "Successfully installed bert-score-0.3.13 rouge-score-0.1.2 sentence-transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-score sentence-transformers nltk rouge-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TjsMd06qR3lW",
        "outputId": "8ad6370e-ff98-4c02-a079-da9cc25be01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_cURU5cOqpSx",
        "outputId": "3091146a-5aeb-4a26-a7f6-91b5a2890697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting whisper_timestamped\n",
            "  Downloading whisper_timestamped-1.15.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from whisper_timestamped) (3.0.11)\n",
            "Collecting dtw-python (from whisper_timestamped)\n",
            "  Downloading dtw_python-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m681.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (from whisper_timestamped) (20231117)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dtw-python->whisper_timestamped) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from dtw-python->whisper_timestamped) (1.26.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper_timestamped) (0.60.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper_timestamped) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper_timestamped) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper_timestamped) (10.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper_timestamped) (0.7.0)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper_timestamped) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper->whisper_timestamped) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper->whisper_timestamped) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper->whisper_timestamped) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper->whisper_timestamped) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper_timestamped) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper->whisper_timestamped) (12.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper_timestamped) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper_timestamped) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper_timestamped) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper_timestamped) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper->whisper_timestamped) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper->whisper_timestamped) (1.3.0)\n",
            "Downloading whisper_timestamped-1.15.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m798.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dtw_python-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dtw-python, whisper_timestamped\n",
            "Successfully installed dtw-python-1.5.1 whisper_timestamped-1.15.4\n"
          ]
        }
      ],
      "source": [
        "pip install whisper_timestamped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "shijfBAK5ST1",
        "outputId": "e72c6dc1-08a8-402f-ed14-49108cb86f63",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.34.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n",
            "MoviePy - Writing audio in output_audio.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Building video video_without_audio.mp4.\n",
            "Moviepy - Writing video video_without_audio.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t: 100%|█████████▉| 8561/8562 [01:36<00:00, 121.90it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/Hindi 1.mp4, 691200 bytes wanted but 0 bytes read,at frame 8561/8562, at time 342.44/342.45 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready video_without_audio.mp4\n",
            "Extraction complete.\n",
            "Conversion complete.\n",
            "Transcribing audio track\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n",
            "100%|██████████| 34245/34245 [35:50<00:00, 15.92frames/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error transcribing audio: string indices must be integers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub moviepy\n",
        "import moviepy.editor as mp\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "\n",
        "def extract_audio_and_video(video_path, audio_output_path, video_output_path_without_audio):\n",
        "    try:\n",
        "        video = mp.VideoFileClip(video_path)\n",
        "\n",
        "        # Save original audio\n",
        "        audio = video.audio\n",
        "        audio.write_audiofile(audio_output_path)\n",
        "\n",
        "        # Save video without audio\n",
        "        video_without_audio = video.set_audio(None)\n",
        "        video_without_audio.write_videofile(video_output_path_without_audio, codec='libx264', audio_codec='aac')\n",
        "\n",
        "        print(\"Extraction complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "def mp3_to_wav(mp3_path, wav_path):\n",
        "    try:\n",
        "        sound = AudioSegment.from_mp3(mp3_path)\n",
        "        sound.export(wav_path, format=\"wav\")\n",
        "        print(\"Conversion complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "def transcribe_audio(audio_file, source_language):\n",
        "    try:\n",
        "        print(\"Transcribing audio track\")\n",
        "        model = whisper.load_model(\"medium\")\n",
        "        trans = model.transcribe(audio_file, language=source_language, verbose=False)\n",
        "        # Remove timestamps\n",
        "        trans_text = \" \".join(word['word'] for word in trans)\n",
        "        return trans_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing audio: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_path = input(\"Enter the path of the original video: \")\n",
        "    audio_output_path_mp3 = \"output_audio.mp3\"\n",
        "    video_output_path_without_audio = \"video_without_audio.mp4\"\n",
        "    audio_output_path_wav = \"output_audio.wav\"\n",
        "    source_language = input(\"Enter the language code of the audio (e.g., en-US for English-US): \")\n",
        "\n",
        "    # Extract audio and video without audio\n",
        "    extract_audio_and_video(video_path, audio_output_path_mp3, video_output_path_without_audio)\n",
        "\n",
        "    # Convert mp3 to wav\n",
        "    mp3_to_wav(audio_output_path_mp3, audio_output_path_wav)\n",
        "\n",
        "    # Transcribe audio\n",
        "    transcribed_text = transcribe_audio(audio_output_path_wav, source_language)\n",
        "    if transcribed_text:\n",
        "        print(\"Transcription:\")\n",
        "        print(transcribed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha_auATKVyeu"
      },
      "source": [
        "**TRANSCRIPT OF ORIGINAL LANGUAGE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "collapsed": true,
        "id": "HbWxmCkB6DX9",
        "outputId": "d1b22e3f-4f8f-4b1f-fcab-39798078f30a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'whisper_timestamped'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1fa7b588be8a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper_timestamped\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maudio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/output_audio.mp3'\u001b[0m  \u001b[0;31m# Updated audio path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtranscript_with_timestamps_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transcript_with_timestamps.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtranscript_without_timestamps_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transcript_without_timestamps.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whisper_timestamped'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "import whisper_timestamped as whisper\n",
        "\n",
        "audio_path = '/content/output_audio.mp3'  # Updated audio path\n",
        "transcript_with_timestamps_path = \"transcript_with_timestamps.txt\"\n",
        "transcript_without_timestamps_path = \"transcript_without_timestamps.txt\"\n",
        "\n",
        "try:\n",
        "    # Load audio and model\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    model = whisper.load_model(\"small\")\n",
        "\n",
        "    # Perform transcription with timestamps\n",
        "    result = whisper.transcribe(model, audio, language=\"hi\")\n",
        "\n",
        "    # Write transcript with timestamps\n",
        "    with open(transcript_with_timestamps_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "        for segment in result['segments']:\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            text = segment['text'].strip()\n",
        "            txt_file.write(f\"{start_time:.2f} --> {end_time:.2f}\\n{text}\\n\")\n",
        "            print(f\"{start_time:.2f} --> {end_time:.2f}\\n{text}\")\n",
        "\n",
        "    print(\"Transcription with timestamps saved to:\", transcript_with_timestamps_path)\n",
        "\n",
        "    # Write transcript without timestamps\n",
        "    with open(transcript_without_timestamps_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "        for segment in result['segments']:\n",
        "            text = segment['text'].strip()\n",
        "            txt_file.write(text + \"\\n\")\n",
        "            print(text)\n",
        "\n",
        "    print(\"Transcript without timestamps saved to:\", transcript_without_timestamps_path)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGf2y-9aVuzA"
      },
      "source": [
        "**TRANSLATION CODE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XkBlqco36ruj",
        "outputId": "f2f9d380-5c04-4afc-e003-39d6e7b04233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the target language code (e.g., 'fr' for French, 'es' for Spanish): ja\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26782/26782 [00:18<00:00, 1433.25frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey everyone, welcome to another and already familiar to many of you speak\n",
            "English with me video and to our new filming spot. Well anyways, in this video\n",
            "you will practice your speaking with me. As usual we will have a dialogue where\n",
            "one line will be mine and the next one will be yours. I'm gonna say my line and\n",
            "then you will read your line from the screen out loud as if you were answering\n",
            "me and then vice versa. This is going to be a casual conversation between friends\n",
            "that are making plans. Apart from working on your speaking skills you might also\n",
            "pick up a phrase or two and improve your vocabulary. First you will listen to and\n",
            "watch the full dialogue and then we'll proceed to the practicing part. Okay,\n",
            "let's go. Hey Jenna, do you have any plans for this weekend? I don't think so.\n",
            "Why? My friend Angela is coming to town and I'd like to introduce you guys to\n",
            "each other. Remember I told you about her. Yes, I remember you talking about her.\n",
            "She sounds like a really nice person. I'd love to finally meet her in person.\n",
            "She and you have a lot in common. I'm sure you'll love her. Awesome. So what did\n",
            "you have in mind? Did you want to go somewhere? There's gonna be a fair on the\n",
            "King's Hill Farm. I was thinking we could go there on Saturday and then on\n",
            "Sunday to this new place on 45th. Lazy Bird? Yes, that one. I've been meeting to\n",
            "go check it out for a while. Yeah, me too. Perfect. Sounds like a plan. Looking\n",
            "forward to this weekend and meeting your friend.\n",
            "Hey Jenna, do you have any plans for this weekend?\n",
            "My friend Angela is coming to town and I'd like to introduce you guys to each\n",
            "other. Remember I told you about her.\n",
            "She and you have a lot in common. I'm sure you'll love her.\n",
            "There's gonna be a fair on the King's Hill Farm. I was thinking we could go\n",
            "there on Saturday and then on Sunday to this new place on 45th.\n",
            "Yes, that one. I've been meeting to go check it out for a while.\n",
            "Perfect.\n",
            "Okay, great job everyone. Now we switch. You go first.\n",
            "I don't think so. Why?\n",
            "Yes, I remember you talking about her. She sounds like a really nice person. I'd\n",
            "love to finally meet her in person.\n",
            "Awesome. So what did you have in mind? Did you want to go somewhere?\n",
            "Okay.\n",
            "Lazy bird.\n",
            "Yeah, me too.\n",
            "Sounds like a plan. Looking forward to this weekend and meeting your friend.\n",
            "Okay, I hope you enjoyed practicing your speaking with me. If you like this\n",
            "format, please give this video a like, subscribe if you haven't yet and I'll\n",
            "see you in the next one. Bye!\n",
            "Transcription saved to: transcript_without_timestamps.txt\n",
            "Original Text:\n",
            "Hey everyone, welcome to another and already familiar to many of you speak\n",
            "English with me video and to our new filming spot. Well anyways, in this video\n",
            "you will practice your speaking with me. As usual we will have a dialogue where\n",
            "one line will be mine and the next one will be yours. I'm gonna say my line and\n",
            "then you will read your line from the screen out loud as if you were answering\n",
            "me and then vice versa. This is going to be a casual conversation between friends\n",
            "that are making plans. Apart from working on your speaking skills you might also\n",
            "pick up a phrase or two and improve your vocabulary. First you will listen to and\n",
            "watch the full dialogue and then we'll proceed to the practicing part. Okay,\n",
            "let's go. Hey Jenna, do you have any plans for this weekend? I don't think so.\n",
            "Why? My friend Angela is coming to town and I'd like to introduce you guys to\n",
            "each other. Remember I told you about her. Yes, I remember you talking about her.\n",
            "She sounds like a really nice person. I'd love to finally meet her in person.\n",
            "She and you have a lot in common. I'm sure you'll love her. Awesome. So what did\n",
            "you have in mind? Did you want to go somewhere? There's gonna be a fair on the\n",
            "King's Hill Farm. I was thinking we could go there on Saturday and then on\n",
            "Sunday to this new place on 45th. Lazy Bird? Yes, that one. I've been meeting to\n",
            "go check it out for a while. Yeah, me too. Perfect. Sounds like a plan. Looking\n",
            "forward to this weekend and meeting your friend.\n",
            "Hey Jenna, do you have any plans for this weekend?\n",
            "My friend Angela is coming to town and I'd like to introduce you guys to each\n",
            "other. Remember I told you about her.\n",
            "She and you have a lot in common. I'm sure you'll love her.\n",
            "There's gonna be a fair on the King's Hill Farm. I was thinking we could go\n",
            "there on Saturday and then on Sunday to this new place on 45th.\n",
            "Yes, that one. I've been meeting to go check it out for a while.\n",
            "Perfect.\n",
            "Okay, great job everyone. Now we switch. You go first.\n",
            "I don't think so. Why?\n",
            "Yes, I remember you talking about her. She sounds like a really nice person. I'd\n",
            "love to finally meet her in person.\n",
            "Awesome. So what did you have in mind? Did you want to go somewhere?\n",
            "Okay.\n",
            "Lazy bird.\n",
            "Yeah, me too.\n",
            "Sounds like a plan. Looking forward to this weekend and meeting your friend.\n",
            "Okay, I hope you enjoyed practicing your speaking with me. If you like this\n",
            "format, please give this video a like, subscribe if you haven't yet and I'll\n",
            "see you in the next one. Bye!\n",
            "\n",
            "\n",
            "Translated Text:\n",
            "みなさん、別の人へようこそ、すでに多くの人に馴染みのある\n",
            "私と私たちの新しい撮影スポットへの英語。とにかく、このビデオで\n",
            "あなたは私と話すことを練習します。いつものように、私たちは対話をするでしょう\n",
            "1つの行が私のもので、次の行はあなたのものです。私は自分のラインを言うつもりです\n",
            "次に、あなたが答えているかのように大声で画面からあなたのラインを読みます\n",
            "私、そしてその逆も同様です。これは友達の間のカジュアルな会話になるでしょう\n",
            "それは計画を立てています。APあなたのスピーキングスキルに取り組むことからあなたもそうかもしれません\n",
            "フレーズを1つまたは2つ受け取り、語彙を改善します。最初に聴きます\n",
            "完全な対話を見てから、練習部分に進みます。わかった、\n",
            "さあ行こう。ねえジェナ、今週末の予定はありますか？私はそうは思わない。\n",
            "なぜ？私の友人のアンジェラが町に来ています、そして私はあなたたちをに紹介したいです\n",
            "お互い。私は彼女についてあなたに言ったことを忘れないでください。はい、私はあなたが彼女について話していることを覚えています。\n",
            "彼女は本当にいい人のように聞こえます。私は大好きですついに彼女に直接会うこと。\n",
            "彼女とあなたには多くの共通点があります。私はあなたが彼女を愛していると確信しています。素晴らしい。それで何をしましたか\n",
            "あなたは念頭に置いていますか？どこかに行きたいですか？フェアがあります\n",
            "キングスヒルファーム。私は土曜日にそこに行くことができると思っていました\n",
            "45位のこの新しい場所への日曜日。怠zyな鳥？はい、それは1つです。私は会ってきました\n",
            "しばらくチェックしてください。ええ、私も。完璧。計画のように聞こえます。見ている\n",
            "今週末に転向し、あなたの友人に会います。\n",
            "ねえジェナ、あなたは持っていますか今週末の計画はありますか？\n",
            "私の友人のアンジェラが町に来ています、そして私はあなたたちにそれぞれを紹介したいです\n",
            "他の。私は彼女についてあなたに言ったことを忘れないでください。\n",
            "彼女とあなたには多くの共通点があります。私はあなたが彼女を愛していると確信しています。\n",
            "キングスヒルファームにはフェアがあります。私たちは行くことができると思っていました\n",
            "土曜日に、そして日曜日に45位にこの新しい場所に行きます。\n",
            "はい、それは1つです。私はそれをしばらくチェックしに行くために会ってきました。\n",
            "完璧。\n",
            "さて、みんな素晴らしい仕事です。今、私たちは切り替えます。あなたは最初に行きます。\n",
            "私はそうは思わない。なぜ？\n",
            "はい、彼女について話しているのを覚えています。彼女は本当にいい人のように聞こえます。私は\n",
            "ついに彼女に直接会うのが大好きです。\n",
            "素晴らしい。それで、あなたは何を念頭に置いていましたか？どこかに行きたいですか？\n",
            "わかった。\n",
            "怠zyな鳥。\n",
            "ええ、私も。\n",
            "計画のように聞こえます。今週末を楽しみにして、あなたの友達に会ってください。\n",
            "さて、私と一緒に話すことを練習するのを楽しんでいただければ幸いです。あなたがこれが好きなら\n",
            "フォーマット、このビデオを同様にしてください。まだ行っていない場合は購読してください。\n",
            "次のものでお会いしましょう。さよなら！\n",
            "Translated text saved to: /content/india_translated.txt\n",
            "Text-to-speech conversion saved to: temp_out.mp3\n",
            "Moviepy - Building video /content/output_video_with_audio.mp4.\n",
            "MoviePy - Writing audio in output_video_with_audioTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video /content/output_video_with_audio.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/output_video_with_audio.mp4\n",
            "Video with merged audio saved successfully.\n",
            "Execution time: 144.22437524795532 seconds\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "from gtts import gTTS\n",
        "from googletrans import Translator\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import whisper_timestamped as whisper\n",
        "\n",
        "def transcribe_audio(audio_path, transcript_path):\n",
        "    try:\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        model = whisper.load_model(\"small\")\n",
        "\n",
        "        result = whisper.transcribe(model, audio, language=\"hi\")\n",
        "\n",
        "        # Write transcript to text file without timestamps\n",
        "        with open(transcript_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                txt_file.write(text + \"\\n\")\n",
        "                print(text)\n",
        "\n",
        "        print(\"Transcription saved to:\", transcript_path)\n",
        "        return transcript_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def translate_text(text, target_language):\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(text, dest=target_language)\n",
        "    return translation.text\n",
        "\n",
        "def translate_paragraph(paragraph, target_language):\n",
        "    translated_paragraph = \"\"\n",
        "    chunks = [paragraph[i:i+500] for i in range(0, len(paragraph), 500)]\n",
        "\n",
        "    def translate_chunk(chunk):\n",
        "        return translate_text(chunk, target_language)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        translated_chunks = executor.map(translate_chunk, chunks)\n",
        "\n",
        "    for translated_chunk in translated_chunks:\n",
        "        translated_paragraph += translated_chunk\n",
        "\n",
        "    return translated_paragraph\n",
        "\n",
        "def save_to_file(translated_text, file_path):\n",
        "    try:\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(translated_text.strip())\n",
        "        print(\"Translated text saved to:\", file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while saving to file: {e}\")\n",
        "\n",
        "def text_to_speech(translated_text, output_audio_path, target_language):\n",
        "    tts = gTTS(text=translated_text, lang=target_language)\n",
        "    tts.save(output_audio_path)\n",
        "    print(\"Text-to-speech conversion saved to:\", output_audio_path)\n",
        "    return output_audio_path\n",
        "\n",
        "def merge_audio_with_video(input_video_path, audio_path, output_video_path):\n",
        "    video_clip = VideoFileClip(input_video_path)\n",
        "    audio_clip = AudioFileClip(audio_path)\n",
        "\n",
        "    duration_diff = video_clip.duration - audio_clip.duration\n",
        "    if duration_diff > 0:\n",
        "        speed_factor = audio_clip.duration / video_clip.duration\n",
        "        audio_clip = audio_clip.speedx(speed_factor)\n",
        "    elif duration_diff < 0:\n",
        "        speed_factor = video_clip.duration / audio_clip.duration\n",
        "        video_clip = video_clip.speedx(speed_factor)\n",
        "\n",
        "    video_clip = video_clip.set_audio(audio_clip)\n",
        "    video_clip.write_videofile(output_video_path, codec='libx264', audio_codec='aac')\n",
        "\n",
        "    video_clip.close()\n",
        "    audio_clip.close()\n",
        "\n",
        "def main():\n",
        "    audio_path = '/content/output_audio.mp3'  # Updated audio path\n",
        "    transcript_path = \"transcript_without_timestamps.txt\"\n",
        "    translated_text_path = \"/content/india_translated.txt\"\n",
        "    target_language = input(\"Enter the target language code (e.g., 'fr' for French, 'es' for Spanish): \")\n",
        "    input_video_path = \"/content/video_without_audio.mp4\"\n",
        "    output_video_path = \"/content/output_video_with_audio.mp4\"\n",
        "    output_audio_path = \"temp_out.mp3\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 1: Transcribe audio\n",
        "    transcript = transcribe_audio(audio_path, transcript_path)\n",
        "\n",
        "    if transcript:\n",
        "        with open(transcript, 'r', encoding='utf-8') as file:\n",
        "            paragraph = file.read()\n",
        "        print(\"Original Text:\")\n",
        "        print(paragraph)\n",
        "\n",
        "        # Step 2: Translate text\n",
        "        translated_paragraph = translate_paragraph(paragraph, target_language)\n",
        "        if translated_paragraph:\n",
        "            print(\"\\nTranslated Text:\")\n",
        "            print(translated_paragraph)\n",
        "\n",
        "            # Step 3: Save translated text\n",
        "            save_to_file(translated_paragraph, translated_text_path)\n",
        "\n",
        "            # Step 4: Text-to-Speech conversion\n",
        "            tts_audio = text_to_speech(translated_paragraph, output_audio_path, target_language)\n",
        "\n",
        "            # Step 5: Merge audio with video\n",
        "            if os.path.exists(input_video_path) and os.path.exists(tts_audio):\n",
        "                merge_audio_with_video(input_video_path, tts_audio, output_video_path)\n",
        "                print(\"Video with merged audio saved successfully.\")\n",
        "            else:\n",
        "                print(\"Input video or audio file not found.\")\n",
        "        else:\n",
        "            print(\"Translation failed.\")\n",
        "    else:\n",
        "        print(\"Text extraction failed.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Execution time:\", end_time - start_time, \"seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKJLetlGVn60"
      },
      "source": [
        "**BERT SCORE ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "ba2efa5cf5a241fbad1cb2e385ef2149",
            "253e299aaa754e9eb64fb7362564ce0e",
            "8d71c118eb4a48cd9656fe4dea89e8d8",
            "c66899e3f6214b8f9b9d76c49c315cd9",
            "78608281a92342a1a607f5878175741b",
            "82c97788bdd143339d798980b0cbf438",
            "afb809071827447cb5399c4b580aa2f0",
            "12db83a6c01c4a15aeff1759460e2c81",
            "74dccebad14d4a40b133409f365c1b94",
            "689eabe7a7104a62b642b52fd09f5d56",
            "ba3c59096b60489cb4765640dcba876a",
            "1437b57d34fa4e6ab619ecc082cd7b2a",
            "a105947868bd4ab4b15a0d4f3b1c25a3",
            "456ccd933ce94b37890258110f3ef442",
            "f0cabed56fd4412d972be4dfce0b0fc5",
            "371e619c9df045ba87fdc67c981ee0ec",
            "778d1cc3cc38441a8352fe1e8b852b7c",
            "be1989a3938e4f27a0e03d7d6421ce97",
            "20d0ac7955884249bad19d846eac3b34",
            "186e950c40ef42099a9bdd9486e15ed9",
            "a78d5af25af344bf8168ec83e7832415",
            "60eed80653d644de9a36d65e38591681"
          ]
        },
        "collapsed": true,
        "id": "8zbBylM92Mlr",
        "outputId": "1ba3b7c2-446c-4d36-adfb-4d1545ccb777"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference Text:\n",
            "\n",
            "Hey everyone, welcome to another and already familiar to many of you speak English with me video and to our new filming spot. Well anyways, in this video you will practice your speaking with me. As usual we will have a dialogue where one line will be mine and the next one will be yours. I'm gonna say my line and then you will read your line from the screen out loud as if you were answering me and then vice versa. This is going to be a casual conversation between friends that are making plans. Apart from working on your speaking skills you might also pick up a phrase or two and improve your vocabulary. First you will listen to and watch the full dialogue and then we'll proceed to the practicing part. Okay, let's go. Hey Jenna, do you have any plans for this weekend? I don't think so. Why? My friend Angela is coming to town and I'd like to introduce you guys to each other. Remember I told you about her. Yes, I remember you talking about her. She sounds like a really nice person. I'd love to finally meet her in person. She and you have a lot in common. I'm sure you'll love her. Awesome. So what did you have in mind? Did you want to go somewhere? There's gonna be a fair on the King's Hill Farm. I was thinking we could go there on Saturday and then on Sunday to this new place on 45th. Lazy Bird? Yes, that one. I've been meaning to go check it out for a while. Yeah, me too. Perfect. Sounds like a plan. Looking forward to this weekend and meeting your friend. Hey Jenna, do you have any plans for this weekend? My friend Angela is coming to town and I'd like to introduce you guys to each other. Remember I told you about her. She and you have a lot in common. I'm sure you'll love her. There's gonna be a fair on the King's Hill Farm. I was thinking we could go there on Saturday and then on Sunday to this new place on 45th. Yes, that one. I've been meaning to go check it out for a while. Perfect. Okay, great job everyone. Now we switch. You go first. I don't think so. Why? Yes, I remember you talking about her. She sounds like a really nice person. I'd love to finally meet her in person. Awesome. So what did you have in mind? Did you want to go somewhere? Lazy Bird. Yeah, me too. Sounds like a plan. Looking forward to this weekend and meeting your friend. Okay, I hope you enjoyed practicing your speaking with me. If you like this format, please give this video a like, subscribe if you haven't yet, and I'll see you in the next one. Bye!\n",
            "\n",
            "\n",
            "Translated Text:\n",
            "\n",
            "みなさん、別の人へようこそ、すでに多くの人に馴染みのある私と私たちの新しい撮影スポットへの英語、とにかく、このビデオであなたは私と話すことを練習します、いつものように、私たちは対話をするでしょう、1つの行が私のもので、次の行はあなたのものです、私は自分のラインを言うつもりです、次に、あなたが答えているかのように大声で画面からあなたのラインを読みます、私、そしてその逆も同様です、これは友達の間のカジュアルな会話になるでしょう、それは計画を立てています、あなたのスピーキングスキルに取り組むことからフレーズを1つまたは2つ受け取り、語彙を改善します、最初に完全な対話を見てから、練習部分に進みます、わかった、さあ行こう、ねえジェナ、今週末の予定はありますか、私はそうは思わない、なぜ、私の友人のアンジェラが町に来ています、そして私はあなたたちを紹介したいです、お互い、私は彼女についてあなたに言ったことを忘れないでください、はい、私はあなたが彼女について話していることを覚えています、彼女は本当にいい人のように聞こえます、私は大好きですついに彼女に直接会うこと、彼女とあなたには多くの共通点があります、私はあなたが彼女を愛していると確信しています、素晴らしい、それで何をしましたか、どこかに行きたいですか、フェアがあります、キングスヒルファーム、私は土曜日にそこに行くことができると思っていました、45位のこの新しい場所への日曜日、怠けた鳥、はい、それは1つです、私は会ってきました、しばらくチェックしてください、ええ、私も、完璧、計画のように聞こえます、今週末を楽しみにして、あなたの友達に会います、さて、みんな素晴らしい仕事です、今、私たちは切り替えます、あなたは最初に行きます、私はそうは思わない、なぜ、はい、彼女について話しているのを覚えています、彼女は本当にいい人のように聞こえます、ついに彼女に直接会うのが大好きです、素晴らしい、それで、あなたは何を念頭に置いていましたか、どこかに行きたいですか、わかった、怠けた鳥、ええ、私も、計画のように聞こえます、今週末を楽しみにして、あなたの友達に会ってください、さて、私と一緒に話すことを練習するのを楽しんでいただければ幸いです、あなたがこれが好きなら、このビデオを同様にしてください、まだ行っていない場合は購読してください、次のものでお会いしましょう、さよなら!\n",
            "\n",
            "\n",
            "Number of reference sentences: 58\n",
            "Number of candidate sentences: 1\n",
            "Warning: Different number of sentences (58 vs 1)\n",
            "Aligned sentences for evaluation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba2efa5cf5a241fbad1cb2e385ef2149",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1437b57d34fa4e6ab619ecc082cd7b2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 0.28 seconds, 3.57 sentences/sec\n",
            "\n",
            "Average BERTScore F1: 0.7270283102989197\n",
            "Average sBERT Similarity: 0.1318056285381317\n",
            "\n",
            "Average BERTScore F1 should be high. Obtained score: 0.7270283102989197\n",
            "Average sBERT Similarity should be high. Obtained score: 0.1318056285381317\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import tempfile\n",
        "from googletrans import Translator\n",
        "from bert_score import score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "\n",
        "# Download NLTK sentence tokenizer model if not already available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the translator and models\n",
        "translator = Translator()\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Function to compute BERTScore for multiple sentences\n",
        "def compute_bertscore(reference_sentences, candidate_sentences):\n",
        "    P, R, F1 = score(candidate_sentences, reference_sentences, lang='en', verbose=True)\n",
        "    return F1.mean().item()\n",
        "\n",
        "# Function to compute sBERT similarity for multiple sentences\n",
        "def compute_sbert(reference_sentences, candidate_sentences):\n",
        "    reference_embeddings = model.encode(reference_sentences, convert_to_tensor=True)\n",
        "    candidate_embeddings = model.encode(candidate_sentences, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(reference_embeddings, candidate_embeddings).diag().tolist()\n",
        "    return sum(similarities) / len(similarities) if similarities else 0\n",
        "\n",
        "# Aggregate evaluation scores for the entire translation\n",
        "def evaluate_translation(reference, candidate):\n",
        "    print(\"Reference Text:\")\n",
        "    print(reference)\n",
        "    print(\"\\nTranslated Text:\")\n",
        "    print(candidate)\n",
        "\n",
        "    # Use NLTK to split texts into sentences\n",
        "    reference_sentences = nltk.sent_tokenize(reference)\n",
        "    candidate_sentences = nltk.sent_tokenize(candidate)\n",
        "\n",
        "    # Strip whitespace and filter out empty sentences\n",
        "    reference_sentences = [sent.strip() for sent in reference_sentences if sent.strip()]\n",
        "    candidate_sentences = [sent.strip() for sent in candidate_sentences if sent.strip()]\n",
        "\n",
        "    print(f\"Number of reference sentences: {len(reference_sentences)}\")\n",
        "    print(f\"Number of candidate sentences: {len(candidate_sentences)}\")\n",
        "\n",
        "    # Ensure both lists have the same number of sentences\n",
        "    if len(reference_sentences) != len(candidate_sentences):\n",
        "        print(f\"Warning: Different number of sentences ({len(reference_sentences)} vs {len(candidate_sentences)})\")\n",
        "        # Align sentences based on content similarity\n",
        "        min_len = min(len(reference_sentences), len(candidate_sentences))\n",
        "        reference_sentences = reference_sentences[:min_len]\n",
        "        candidate_sentences = candidate_sentences[:min_len]\n",
        "        print(\"Aligned sentences for evaluation.\")\n",
        "\n",
        "    # Evaluate with BERTScore\n",
        "    bertscore_f1 = compute_bertscore(reference_sentences, candidate_sentences)\n",
        "    print(f\"\\nAverage BERTScore F1: {bertscore_f1}\")\n",
        "\n",
        "    # Evaluate with sBERT similarity\n",
        "    sbert_similarity = compute_sbert(reference_sentences, candidate_sentences)\n",
        "    print(f\"Average sBERT Similarity: {sbert_similarity}\")\n",
        "\n",
        "    return {\n",
        "        \"BERTScore_F1\": bertscore_f1,\n",
        "        \"sBERT_Similarity\": sbert_similarity\n",
        "    }\n",
        "\n",
        "# Example reference and candidate translations\n",
        "reference_text = \"\"\"\n",
        "Hey everyone, welcome to another and already familiar to many of you speak English with me video and to our new filming spot. Well anyways, in this video you will practice your speaking with me. As usual we will have a dialogue where one line will be mine and the next one will be yours. I'm gonna say my line and then you will read your line from the screen out loud as if you were answering me and then vice versa. This is going to be a casual conversation between friends that are making plans. Apart from working on your speaking skills you might also pick up a phrase or two and improve your vocabulary. First you will listen to and watch the full dialogue and then we'll proceed to the practicing part. Okay, let's go. Hey Jenna, do you have any plans for this weekend? I don't think so. Why? My friend Angela is coming to town and I'd like to introduce you guys to each other. Remember I told you about her. Yes, I remember you talking about her. She sounds like a really nice person. I'd love to finally meet her in person. She and you have a lot in common. I'm sure you'll love her. Awesome. So what did you have in mind? Did you want to go somewhere? There's gonna be a fair on the King's Hill Farm. I was thinking we could go there on Saturday and then on Sunday to this new place on 45th. Lazy Bird? Yes, that one. I've been meaning to go check it out for a while. Yeah, me too. Perfect. Sounds like a plan. Looking forward to this weekend and meeting your friend. Hey Jenna, do you have any plans for this weekend? My friend Angela is coming to town and I'd like to introduce you guys to each other. Remember I told you about her. She and you have a lot in common. I'm sure you'll love her. There's gonna be a fair on the King's Hill Farm. I was thinking we could go there on Saturday and then on Sunday to this new place on 45th. Yes, that one. I've been meaning to go check it out for a while. Perfect. Okay, great job everyone. Now we switch. You go first. I don't think so. Why? Yes, I remember you talking about her. She sounds like a really nice person. I'd love to finally meet her in person. Awesome. So what did you have in mind? Did you want to go somewhere? Lazy Bird. Yeah, me too. Sounds like a plan. Looking forward to this weekend and meeting your friend. Okay, I hope you enjoyed practicing your speaking with me. If you like this format, please give this video a like, subscribe if you haven't yet, and I'll see you in the next one. Bye!\n",
        "\"\"\"\n",
        "\n",
        "# Replace 'candidate_text' with actual translation to be evaluated\n",
        "candidate_text = \"\"\"\n",
        "みなさん、別の人へようこそ、すでに多くの人に馴染みのある私と私たちの新しい撮影スポットへの英語、とにかく、このビデオであなたは私と話すことを練習します、いつものように、私たちは対話をするでしょう、1つの行が私のもので、次の行はあなたのものです、私は自分のラインを言うつもりです、次に、あなたが答えているかのように大声で画面からあなたのラインを読みます、私、そしてその逆も同様です、これは友達の間のカジュアルな会話になるでしょう、それは計画を立てています、あなたのスピーキングスキルに取り組むことからフレーズを1つまたは2つ受け取り、語彙を改善します、最初に完全な対話を見てから、練習部分に進みます、わかった、さあ行こう、ねえジェナ、今週末の予定はありますか、私はそうは思わない、なぜ、私の友人のアンジェラが町に来ています、そして私はあなたたちを紹介したいです、お互い、私は彼女についてあなたに言ったことを忘れないでください、はい、私はあなたが彼女について話していることを覚えています、彼女は本当にいい人のように聞こえます、私は大好きですついに彼女に直接会うこと、彼女とあなたには多くの共通点があります、私はあなたが彼女を愛していると確信しています、素晴らしい、それで何をしましたか、どこかに行きたいですか、フェアがあります、キングスヒルファーム、私は土曜日にそこに行くことができると思っていました、45位のこの新しい場所への日曜日、怠けた鳥、はい、それは1つです、私は会ってきました、しばらくチェックしてください、ええ、私も、完璧、計画のように聞こえます、今週末を楽しみにして、あなたの友達に会います、さて、みんな素晴らしい仕事です、今、私たちは切り替えます、あなたは最初に行きます、私はそうは思わない、なぜ、はい、彼女について話しているのを覚えています、彼女は本当にいい人のように聞こえます、ついに彼女に直接会うのが大好きです、素晴らしい、それで、あなたは何を念頭に置いていましたか、どこかに行きたいですか、わかった、怠けた鳥、ええ、私も、計画のように聞こえます、今週末を楽しみにして、あなたの友達に会ってください、さて、私と一緒に話すことを練習するのを楽しんでいただければ幸いです、あなたがこれが好きなら、このビデオを同様にしてください、まだ行っていない場合は購読してください、次のものでお会いしましょう、さよなら!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Evaluate translation\n",
        "evaluation_scores = evaluate_translation(reference_text, candidate_text)\n",
        "\n",
        "# Display evaluation scores with descriptive messages\n",
        "if evaluation_scores:\n",
        "    print(f\"\\nAverage BERTScore F1 should be high. Obtained score: {evaluation_scores['BERTScore_F1']}\")\n",
        "    print(f\"Average sBERT Similarity should be high. Obtained score: {evaluation_scores['sBERT_Similarity']}\")\n",
        "else:\n",
        "    print(\"Evaluation could not be completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZluW3MHHVjZm"
      },
      "source": [
        "**WER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvlvesQLHgOT",
        "outputId": "959e1387-7530-4d08-d70b-1efa0d9d171a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate: 0.2074235807860262\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "def wer(reference, hypothesis):\n",
        "    # Preprocess the reference and hypothesis\n",
        "    ref_words = preprocess_text(reference).split()\n",
        "    hyp_words = preprocess_text(hypothesis).split()\n",
        "\n",
        "    # Check if the reference is empty to avoid division by zero\n",
        "    if len(ref_words) == 0:\n",
        "        return float('inf')  # Return infinity or some large value if reference is empty\n",
        "\n",
        "    # Initialize the dynamic programming table\n",
        "    dp = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
        "\n",
        "    # Fill the dynamic programming table\n",
        "    for i in range(len(ref_words) + 1):\n",
        "        for j in range(len(hyp_words) + 1):\n",
        "            if i == 0:\n",
        "                dp[i][j] = j\n",
        "            elif j == 0:\n",
        "                dp[i][j] = i\n",
        "            else:\n",
        "                dp[i][j] = min(dp[i-1][j-1] + (0 if ref_words[i-1] == hyp_words[j-1] else 1),\n",
        "                               dp[i-1][j] + 1,\n",
        "                               dp[i][j-1] + 1)\n",
        "\n",
        "    # Return the WER (normalized edit distance)\n",
        "    return float(dp[len(ref_words)][len(hyp_words)]) / len(ref_words)\n",
        "\n",
        "# Input the Youtube transcript (reference) and the Model transcript (hypothesis)\n",
        "reference = \"\"\"\n",
        "किसी ने बड़े कमाल की बात कही है कि जीवन मिलना भाग्य की बात है, मृत्यु होना समय की बात है, और मृत्यु के बाद भी लोगों के दिलों में जीवित रहना कर्मों की बात है। नमस्ते दोस्तों, मैं हूं राज कार्तिक। एक डॉक्टर साहब अपने अस्पताल में दौड़े चले आ रहे थे, जितनी स्पीड में चल सकते थे, दौड़े चले आ रहे थे। ऑपरेशन थिएटर वाली लॉबी में आए तो उन्हें पांच-छह लोगों ने घेर लिया और सुनाने लगे कि अस्पताल खोल लिया है, अपने आप को क्या समझते हो, अब आ रहे हो, हम इतनी देर से इंतजार कर रहे हैं, आपको कोई सुध नहीं है, आपको कोई चिंता नहीं है, हमारा बच्चा ऑपरेशन थिएटर में है, हम इंतजार करते-करते थक गए, आप अब आ रहे हो। उन्होंने कहा, मुझे जान दीजिए, मुझे कम करना है। वो लोग सुनाने लगे कि डॉक्टर साहब, आपका बेटा होता तो क्या, आप इतने आराम से आते हैं, आप भाग के आते हैं, आप जल्दी आते हैं, आप ये करते हैं, वो करते हैं, बहुत कुछ कह दिया डॉक्टर साहब को। डॉक्टर साहब ने कोई रिएक्ट नहीं किया, वो दौड़ चले गए ऑपरेशन थिएटर में। एक डेढ़ घंटे तक ऑपरेशन चला। उसके बाद जब बाहर निकले तो फिर उन पांच-छह लोगों ने घेर लिया और पूछा कि क्या हुआ। डॉक्टर साहब ने कहा, बच्चा ठीक है, अब जल्द रिकवर करेगा। फिर उन्होंने सवाल किए कि कब डिस्चार्ज होगा, ये होगा। डॉक्टर साहब ने कहा, अब ये सारे सवाल आप नर्स से पूछ लीजिए, मुझे जान दीजिए। और फिर वो स्पीड से आए थे, उससे दोगुनी स्पीड से निकल गए। ये लोग समझने लगे कि डॉक्टर साहब कितने घमंडी हैं, बात तक करने की तमीज नहीं है। कुछ देर बाद नर्स ने बताया कि डॉक्टर साहब बहुत अच्छे हैं। आज सुबह एक रोड एक्सीडेंट में उनके बेटे की मौत हो गई थी, लेकिन वो बिना किसी देरी के दौड़े चले आए, ऑपरेशन थिएटर में गए, आपके बच्चे का ऑपरेशन किया, और अब उसे ठीक कर देंगे, चिंता मत कीजिए। डॉक्टर साहब शाम को अपने बेटे का अंतिम संस्कार करने जा रहे थे। इस कहानी का सार ये है कि बिना किसी की सिचुएशन समझे हम रिएक्ट कर देते हैं, हम स्थिति समझते नहीं हैं, बस अपनी बात थोप देते हैं और बाद में गिल्ट महसूस करने लगते हैं। इसलिए जीवन में कभी भी किसी की सिचुएशन जान बिना कोई भी टिप्पणी मत कीजिएगा। एक बार फिर से वही बात, जो अक्सर आपसे कहता हूं, अच्छा बनिए, बेहतर बनिए, बेहतरीन बनने का प्रयास कीजिए, जीवन में कर दिखाइए कुछ ऐसा कि दुनिया करना चाहे सिर्फ आपके जैसा। देखते रहिए हर सोमवार सुबह 9:30 बजे मेरे साथ एक नई मोटिवेशनल कहानी और हर शुक्रवार सुबह 9:30 बजे लर्निंग सीरीज का एक नया वीडियो।\"\"\"\n",
        "hypothesis = \"\"\"\n",
        "किसी ने बड़े कमाल की बात कही है कि जीवन मिलना भाग्य की बात है, मृत्यु होना समय की बात है, और मृत्यु के बाद भी लोगों के दिलों में जीवित रहना कर्मों की बात है। नमस्ते दोस्तों, मैं हूँ आरजे कार्तिक। एक डॉक्टर अपने अस्पताल में दौड़े चले आ रहे थे, जितनी स्पीड में चल सकते थे, दौड़े चले आ रहे थे। ऑपरेशन थियेटर वाली लॉबी में आए तो पांच-छह लोगों ने घेर लिया और सुनाने लगे कि अस्पताल खोल लिया है, अपने आपको क्या समझते हो, अब आ रहे हो, हम इतनी देर से इंतजार कर रहे हैं, आपको कोई सुध नहीं है, आपको कोई चिंता नहीं है। उन लोगों ने डॉक्टर से कहा कि आपका बेटा होता तो क्या, आप इतने आराम से आते हैं, आप भाग के आते हैं, आप जल्दी आते हैं, आप ये करते हैं, वो करते हैं, बहुत कुछ कह दिया डॉक्टर साहब को। डॉक्टर साहब ने कोई प्रतिक्रिया नहीं की, वो दौड़ चले गए ऑपरेशन थियेटर में। एक डेढ़ घंटे तक ऑपरेशन चला। जब बाहर आए, तो पांच-छह लोगों ने फिर घेर लिया और पूछा कि क्या हुआ। डॉक्टर साहब ने कहा कि बच्चा ठीक है, अब जल्द रिकवर करेगा। उन्होंने सवाल किए कि कब डिस्चार्ज होगा, ये होगा। डॉक्टर साहब ने कहा कि अब ये सारे सवाल आप नर्स से पूछ लीजिए, मुझे जान दीजिए। फिर वो स्पीड से आए थे, उससे दोगुनी स्पीड से निकल गए। इन लोगों को लगा कि डॉक्टर कितने घमंडी हैं, बात तक करने की तमीज नहीं है। कुछ देर बाद नर्स आई और बताया कि डॉक्टर साहब बहुत अच्छे हैं, आज सुबह एक रोड एक्सीडेंट में उनके बेटे की मौत हो गई थी। लेकिन उन्होंने बिना किसी देरी के दौड़ कर आकर ऑपरेशन किया और अब बच्चे को ठीक कर देंगे। डॉक्टर शाम को अपने बेटे का अंतिम संस्कार करने जा रहे थे। इस कहानी का सार यह है कि बिना किसी की स्थिति समझे हम प्रतिक्रिया कर देते हैं और बाद में गिल्ट महसूस करने लगते हैं। इसलिए जीवन में कभी भी किसी की स्थिति जान बिना कोई टिप्पणी मत कीजिए। एक बार फिर से वही बात, जो अक्सर आपसे कहता हूं, अच्छा बनिए, बेहतर बनिए, बेहतरीन बनने का प्रयास कीजिए, जीवन में कर दिखाइए कुछ ऐसा कि दुनिया करना चाहे सिर्फ आपके जैसा। देखते रहिए हर सोमवार सुबह 9:30 बजे मेरे साथ एक नई मोटिवेशनल कहानी और हर शुक्रवार सुबह 9:30 बजे लर्निंग सीरीज का एक नया वीडियो।\n",
        "\"\"\"\n",
        "\n",
        "# Calculate the Word Error Rate (WER)\n",
        "wer_score = wer(reference, hypothesis)\n",
        "print(\"Word Error Rate:\", wer_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zSkPOYdVd7d"
      },
      "source": [
        "**YOUTUBE TRANSCRIPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ES9nvvfREJR6",
        "outputId": "ee539892-c590-41e1-cb34-ed76becdf1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.7.4)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n"
          ]
        }
      ],
      "source": [
        "pip install youtube-transcript-api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ztbDkdwNyPop",
        "outputId": "7e71fa25-82d8-4db6-fb56-8d4fa13ab333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter YouTube URLs (type 'done' to finish):\n",
            "YouTube URL: https://youtu.be/nFYlN5Fayvs?si=cyvPRXu4QQmko-NT\n",
            "YouTube URL: done\n",
            "Enter the language code (e.g., 'es' for Spanish, 'fr' for French): hi\n",
            "Transcript saved as transcripts/nFYlN5Fayvs_transcript_hi.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, VideoUnavailable, NoTranscriptFound\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(filename='transcript_scraper.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def extract_video_id(url):\n",
        "    \"\"\"\n",
        "    Extract the video ID from a YouTube URL.\n",
        "    \"\"\"\n",
        "    video_id = None\n",
        "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\n",
        "    match = re.search(pattern, url)\n",
        "    if match:\n",
        "        video_id = match.group(1)\n",
        "    return video_id\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"\n",
        "    Convert seconds to minutes and seconds format.\n",
        "    \"\"\"\n",
        "    minutes = int(seconds // 60)\n",
        "    seconds = seconds % 60\n",
        "    return f\"{minutes}:{seconds:05.2f}\"\n",
        "\n",
        "def get_transcript(video_id, language_code):\n",
        "    try:\n",
        "        # Fetch the transcript in the specified language\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[language_code])\n",
        "        return transcript\n",
        "    except NoTranscriptFound:\n",
        "        logging.error(f\"No transcript found for video ID {video_id} in language '{language_code}'.\")\n",
        "        return None\n",
        "    except TranscriptsDisabled:\n",
        "        logging.error(f\"Transcripts are disabled for video ID {video_id}.\")\n",
        "        return None\n",
        "    except VideoUnavailable:\n",
        "        logging.error(f\"Video is unavailable for video ID {video_id}.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred for video ID {video_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_transcript(video_id, transcript, output_dir, language_code):\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save the transcript with timestamps to a file\n",
        "    file_path = os.path.join(output_dir, f\"{video_id}_transcript_{language_code}.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for entry in transcript:\n",
        "            if 'start' in entry and 'duration' in entry:\n",
        "                start_time = format_time(entry['start'])\n",
        "                end_time = format_time(entry['start'] + entry['duration'])\n",
        "                f.write(f\"{start_time} - {end_time} - {entry['text']}\\n\")\n",
        "            else:\n",
        "                logging.warning(f\"Missing 'start' or 'duration' in transcript entry for video ID {video_id}.\")\n",
        "    print(f\"Transcript saved as {file_path}\")\n",
        "\n",
        "def process_videos(url_list, output_dir, language_code):\n",
        "    for url in url_list:\n",
        "        video_id = extract_video_id(url)\n",
        "        if video_id:\n",
        "            transcript = get_transcript(video_id, language_code)\n",
        "            if transcript:\n",
        "                save_transcript(video_id, transcript, output_dir, language_code)\n",
        "            else:\n",
        "                logging.error(f\"Failed to get transcript for video ID {video_id} in language '{language_code}'.\")\n",
        "        else:\n",
        "            logging.error(f\"Could not extract video ID from URL: {url}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Allow user to input multiple YouTube URLs\n",
        "    urls = []\n",
        "    print(\"Enter YouTube URLs (type 'done' to finish):\")\n",
        "\n",
        "    while True:\n",
        "        url = input(\"YouTube URL: \")\n",
        "        if url.lower() == 'done':\n",
        "            break\n",
        "        urls.append(url)\n",
        "\n",
        "    if urls:\n",
        "        # Ask the user for the desired language code\n",
        "        language_code = input(\"Enter the language code (e.g., 'es' for Spanish, 'fr' for French): \").strip()\n",
        "\n",
        "        # Specify the directory where transcripts will be saved\n",
        "        output_dir = \"transcripts\"\n",
        "        process_videos(urls, output_dir, language_code)\n",
        "    else:\n",
        "        print(\"No URLs provided.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efyj4E7gVUlf"
      },
      "source": [
        "**BACKUP CODE FOR JAPANESE ONLY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yVBle4rlOd5c",
        "outputId": "2c6b50ec-ca18-499e-abf0-f9509f5b380b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the target language code (e.g., 'fr' for French, 'es' for Spanish): ja\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 39402/39402 [00:29<00:00, 1326.76frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ciao, sono Katy e tu? Piacere!\n",
            "In questa puntata di Super Easy Italian, imparerai come presentarti in italiano. Andiamo!\n",
            "Ciao! Ciao! Ciao!\n",
            "Ciao! Piacere! Piacere!\n",
            "Ciao! Buongiorno! Buongiorno a lei! Buongiorno!\n",
            "Come ti chiami? Valerio! Piacere! Piacere mio!\n",
            "Sono Elena, piacere! Piacere! Come ti chiami? Mi chiamo Lodovica! Mi chiamo Marco! Piacere! Piacere mio!\n",
            "Sono Katy, come ti chiami? Mi chiamo Martina! Come si chiama? Salvatore! Piacere! Piacere! Piacere!\n",
            "Sono Alessandra! Dove abiti? Dove abiti? A Roma! Abito a Milano! Dove abiti? Abito a Modena! Abito a Torino! Abito a Milano!\n",
            "Di dove sei? Di dove sei? Vengo da Venezia! Tu di dove sei? Sono inglese! Di dove sei? Sono italiana? Di dove sei? Sono di Roma! Di dove sei? Sono di Venezia!\n",
            "Di dove? Di dove? Sicilia! Di dove? Di Milano!\n",
            "Io sono un attivo di 30, ma vivo a Milano da tantissimi anni, praticamente da 50, 60 anni. Dove abita? Dove abita? A Milano! Abito a Milano? Dove abita? Abito qua a Milano, nel villaggio Maggiolina.\n",
            "Ciao! Io abito a Milano e tu? Qual è la differenza tra abitare e vivere? E quando usiamo il formale?\n",
            "E quando usiamo l'informale? Potrai scoprire la risposta a queste domande negli esercizi di questa settimana e potrai ripassare le frasi utili di questa puntata in modo da poterle utilizzare in modo tranquillo nelle tue conversazioni.\n",
            "Per accedere agli esercizi e tanti altri materiali utili come audio lento, audio veloce, video con e senza sottotitoli, basta far parte della comunità Easy Italian. Per saperne di più, clicca il link in descrizione. Ciao!\n",
            "Che lavoro fai? Studio moda? Ancora a Milano? Che lavoro fai? Lavoro per una compagnia aria. Faccio la studentessa. Cosa studi? Studi ingegneria informatica. Faccio ingegnere meccanico. Sono una studentessa. Cosa studi? Architettura. Che lavoro fa? Adesso niente, sono vecchio. Che lavoro faceva? Spedizioniere.\n",
            "Che lavoro fa? Ormai non lavoro più perché sono una pensionata o tantenne. Auguri. Grazie. Che lavoro faceva? Avevo dei negozi di abbigliamento femminile.\n",
            "Ero un insegnante, adesso sono in pensione.\n",
            "Cosa fai nel tempo libero? Nel tempo libero, pratico sport e mi riposo. Cosa fai nel tempo libero? Mi piace guardare film. Cosa fai nel tempo libero? Mi piace fare le passeggiate, mi piace ascoltare la musica e andare in palestra. Cosa fai nel tempo libero? Nel tempo libero mi piace fare sport.\n",
            "Cosa fai nel tempo libero? Mi piace il calcio e andare a concerti. Cosa fai nel tempo libero? Cosa fai nel tempo libero? Adesso vado in barca. Bello. A qualche viaggio in programma? No, io vivo 7 mesi in Sardegna. Bellissimo. Cosa fai nel tempo libero? Leggo e scrivo.\n",
            "E poi io c'ho anche su. E vado a spasso. Cosa fai nel tempo libero? Poche cose, purtroppo adesso, perché appunto, visto la mia giovane età, faccio fatica fare tante cose, per cui mi limito a rasettare la casa, fare la spesa, da mangiare, qualche passeggiata. E basta.\n",
            "Ciao. Come ti chiami? Piacere. Dove abiti? Di dove sei? Che lavoro fai? Cosa fai nel tempo libero? Buongiorno. Come si chiama? Dove abita? Di dove? Che lavoro fa? Cosa fa?\n",
            "Nel tempo libero? E tu come ti chiami? Dove abiti? Come risponderesti alle domande della puntata di oggi? Scrivilo nei commenti.\n",
            "Sottotitoli e revisione a cura di QTSS\n",
            "Transcription saved to: transcript_without_timestamps.txt\n",
            "Original Text:\n",
            "Ciao, sono Katy e tu? Piacere!\n",
            "In questa puntata di Super Easy Italian, imparerai come presentarti in italiano. Andiamo!\n",
            "Ciao! Ciao! Ciao!\n",
            "Ciao! Piacere! Piacere!\n",
            "Ciao! Buongiorno! Buongiorno a lei! Buongiorno!\n",
            "Come ti chiami? Valerio! Piacere! Piacere mio!\n",
            "Sono Elena, piacere! Piacere! Come ti chiami? Mi chiamo Lodovica! Mi chiamo Marco! Piacere! Piacere mio!\n",
            "Sono Katy, come ti chiami? Mi chiamo Martina! Come si chiama? Salvatore! Piacere! Piacere! Piacere!\n",
            "Sono Alessandra! Dove abiti? Dove abiti? A Roma! Abito a Milano! Dove abiti? Abito a Modena! Abito a Torino! Abito a Milano!\n",
            "Di dove sei? Di dove sei? Vengo da Venezia! Tu di dove sei? Sono inglese! Di dove sei? Sono italiana? Di dove sei? Sono di Roma! Di dove sei? Sono di Venezia!\n",
            "Di dove? Di dove? Sicilia! Di dove? Di Milano!\n",
            "Io sono un attivo di 30, ma vivo a Milano da tantissimi anni, praticamente da 50, 60 anni. Dove abita? Dove abita? A Milano! Abito a Milano? Dove abita? Abito qua a Milano, nel villaggio Maggiolina.\n",
            "Ciao! Io abito a Milano e tu? Qual è la differenza tra abitare e vivere? E quando usiamo il formale?\n",
            "E quando usiamo l'informale? Potrai scoprire la risposta a queste domande negli esercizi di questa settimana e potrai ripassare le frasi utili di questa puntata in modo da poterle utilizzare in modo tranquillo nelle tue conversazioni.\n",
            "Per accedere agli esercizi e tanti altri materiali utili come audio lento, audio veloce, video con e senza sottotitoli, basta far parte della comunità Easy Italian. Per saperne di più, clicca il link in descrizione. Ciao!\n",
            "Che lavoro fai? Studio moda? Ancora a Milano? Che lavoro fai? Lavoro per una compagnia aria. Faccio la studentessa. Cosa studi? Studi ingegneria informatica. Faccio ingegnere meccanico. Sono una studentessa. Cosa studi? Architettura. Che lavoro fa? Adesso niente, sono vecchio. Che lavoro faceva? Spedizioniere.\n",
            "Che lavoro fa? Ormai non lavoro più perché sono una pensionata o tantenne. Auguri. Grazie. Che lavoro faceva? Avevo dei negozi di abbigliamento femminile.\n",
            "Ero un insegnante, adesso sono in pensione.\n",
            "Cosa fai nel tempo libero? Nel tempo libero, pratico sport e mi riposo. Cosa fai nel tempo libero? Mi piace guardare film. Cosa fai nel tempo libero? Mi piace fare le passeggiate, mi piace ascoltare la musica e andare in palestra. Cosa fai nel tempo libero? Nel tempo libero mi piace fare sport.\n",
            "Cosa fai nel tempo libero? Mi piace il calcio e andare a concerti. Cosa fai nel tempo libero? Cosa fai nel tempo libero? Adesso vado in barca. Bello. A qualche viaggio in programma? No, io vivo 7 mesi in Sardegna. Bellissimo. Cosa fai nel tempo libero? Leggo e scrivo.\n",
            "E poi io c'ho anche su. E vado a spasso. Cosa fai nel tempo libero? Poche cose, purtroppo adesso, perché appunto, visto la mia giovane età, faccio fatica fare tante cose, per cui mi limito a rasettare la casa, fare la spesa, da mangiare, qualche passeggiata. E basta.\n",
            "Ciao. Come ti chiami? Piacere. Dove abiti? Di dove sei? Che lavoro fai? Cosa fai nel tempo libero? Buongiorno. Come si chiama? Dove abita? Di dove? Che lavoro fa? Cosa fa?\n",
            "Nel tempo libero? E tu come ti chiami? Dove abiti? Come risponderesti alle domande della puntata di oggi? Scrivilo nei commenti.\n",
            "Sottotitoli e revisione a cura di QTSS\n",
            "\n",
            "Error in translation: sequence item 31: expected str instance, NoneType found\n",
            "\n",
            "Translated Text:\n",
            "こんにちは、私はケイティとあなた？喜び！\n",
            "Super Easy Italianのこのエピソードでは、イタリア語で自分自身を提示する方法を学びます。さあ行こう！\n",
            "こんにちは！こんにちは！こんにちは！\n",
            "こんにちは！喜び！喜び！\n",
            "こんにちは！おはよう！彼女におはようございます！おはよう！\n",
            "あなたの名前は何ですか？ヴァレリオ！喜び！どういたしまして！\n",
            "私はエレナです、喜びです！喜び！あなたの名前は何ですか？私の名前はロドビカです！私の名前はマルコです！喜び！どういたしまして！\n",
            "私はケイティです、あなたの名前は何ですか？私の名前はマルティナです！彼の名前は何ですか？救世主！喜び！喜び！喜び！\n",
            "私はアレッサンドラです！どこに住んでいますか？ここでabまたはミラノとあなたに住んでいますか？生活と生活の違いは何ですか？そして、いつフォーマルを使用しますか？\n",
            "そして、いつ私たちは非公式を使用しますか？今週の演習でこれらの質問に対する答えを見つけることができ、このエピソードの有用なフレーズを確認して、会話で静かに使用できるようにすることができます。\n",
            "エクササイズや、ゆっくりとしたオーディオ、高速オーディオ、サブタイトルの有無にかかわらずビデオなど、他の多くの有用な資料にアクセスするには、簡単なイタリアのコミュニティの一部になります。のためにペルネは、説明のリンクをクリックします。こんにちは！\n",
            "あなたはどんな仕事をしますか？ファッションの勉強？まだミラノにいますか？あなたはどんな仕事をしますか？私は航空会社で働いています。私は学生を作ります。あなたは何を勉強してますか？コンピューター工学研究。私は機械エンジニアをやっています。私は学生です。あなたは何を勉強してますか？建築。それはどんな仕事をしますか？今は何も、私は年をとっています。どんな仕事をしましたか？荷送人。\n",
            "それはどんな仕事をしますか？私は年金受給者またはタンチェンなので、今はもう働いていません。おめでとう。ありがとう。どんな仕事をしましたか？私はいくつかの店を持っていました女性の愛。\n",
            "私は先生でしたが、今は引退しています。\n",
            "自由時間に何をしますか？あなたの自由な時間に、実用的なスポーツと私は休んでいます。自由時間に何をしますか？私は映画を見るのが好きです。自由時間に何をしますか？私は散歩をするのが好きで、音楽を聴いてジムに行くのが好きです。自由時間に何をしますか？あなたの自由な時間に私はスポーツをするのが好きです。\n",
            "自由時間に何をしますか？私はサッカーが好きで、コンサートに行きます。自由時間に何をしますか？自由時間に何をしますか？今、私はボートで行きます。ハンサム。いくつかのスケジュールされた旅行に？いいえ、私はサルデーニャに7ヶ月住んでいます。美しい。自由時間に何をしますか？私は読み書きします。\n",
            "そして、私も持っています。そして、私は散歩に行きます。自由時間に何をしますか？残念なことに、実際には幼い年齢を考えると、私は多くのことをするのに苦労しているので、家、買い物、食べる、散歩をするだけなので、ほとんど何もありません。そしてそれだけです。\n",
            "こんにちは。あなたの名前は何ですか？喜び。どこに住んでいますか？どこの出身ですか？あなたはどんな仕事をしますか？自由時間に何をしますか？良さorno。彼の名前は何ですか？どこに住んでいますか？どこ？それはどんな仕事をしますか？それは何をしますか？\n",
            "自由時間に？そして、あなたの名前は何ですか？どこに住んでいますか？今日のエピソードについての質問にどのように答えますか？コメントに書いてください。\n",
            "QTSSによる字幕とレビュー\n",
            "Translated text saved to: /content/india_translated.txt\n",
            "Text-to-speech conversion saved to: temp_out.mp3\n",
            "Error in merging audio with video: 'AudioFileClip' object has no attribute 'speedx'\n",
            "Execution time: 71.57705450057983 seconds\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "from gtts import gTTS\n",
        "from googletrans import Translator\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import whisper_timestamped as whisper\n",
        "\n",
        "def transcribe_audio(audio_path, transcript_path):\n",
        "    try:\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "        model = whisper.load_model(\"small\")\n",
        "\n",
        "        result = whisper.transcribe(model, audio, language=\"it\")\n",
        "\n",
        "        with open(transcript_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            for segment in result['segments']:\n",
        "                text = segment['text'].strip()\n",
        "                txt_file.write(text + \"\\n\")\n",
        "                print(text)\n",
        "\n",
        "        print(\"Transcription saved to:\", transcript_path)\n",
        "        return transcript_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error in transcription: {e}\")\n",
        "        return None\n",
        "\n",
        "def translate_text(text, target_language):\n",
        "    translator = Translator()\n",
        "    try:\n",
        "        translation = translator.translate(text, dest=target_language)\n",
        "        if translation.text is None:\n",
        "            raise ValueError(\"Translation returned None\")\n",
        "        return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in translation: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def translate_paragraph(paragraph, target_language):\n",
        "    translated_paragraph = \"\"\n",
        "    chunks = [paragraph[i:i+500] for i in range(0, len(paragraph), 500)]\n",
        "\n",
        "    def translate_chunk(chunk):\n",
        "        result = translate_text(chunk, target_language)\n",
        "        if result is None:\n",
        "            result = \"\"  # Default to empty string if translation fails\n",
        "        return result\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        translated_chunks = executor.map(translate_chunk, chunks)\n",
        "\n",
        "    for translated_chunk in translated_chunks:\n",
        "        translated_paragraph += translated_chunk\n",
        "\n",
        "    return translated_paragraph\n",
        "\n",
        "def save_to_file(translated_text, file_path):\n",
        "    try:\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(translated_text.strip())\n",
        "        print(\"Translated text saved to:\", file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while saving to file: {e}\")\n",
        "\n",
        "def text_to_speech(translated_text, output_audio_path, target_language):\n",
        "    try:\n",
        "        tts = gTTS(text=translated_text, lang=target_language)\n",
        "        tts.save(output_audio_path)\n",
        "        print(\"Text-to-speech conversion saved to:\", output_audio_path)\n",
        "        return output_audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred in text-to-speech conversion: {e}\")\n",
        "        return None\n",
        "\n",
        "def merge_audio_with_video(input_video_path, audio_path, output_video_path):\n",
        "    try:\n",
        "        video_clip = VideoFileClip(input_video_path)\n",
        "        audio_clip = AudioFileClip(audio_path)\n",
        "\n",
        "        duration_diff = video_clip.duration - audio_clip.duration\n",
        "        if duration_diff > 0:\n",
        "            speed_factor = audio_clip.duration / video_clip.duration\n",
        "            audio_clip = audio_clip.speedx(speed_factor)\n",
        "        elif duration_diff < 0:\n",
        "            speed_factor = video_clip.duration / audio_clip.duration\n",
        "            video_clip = video_clip.speedx(speed_factor)\n",
        "\n",
        "        video_clip = video_clip.set_audio(audio_clip)\n",
        "        video_clip.write_videofile(output_video_path, codec='libx264', audio_codec='aac')\n",
        "\n",
        "        video_clip.close()\n",
        "        audio_clip.close()\n",
        "        print(\"Video with merged audio saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in merging audio with video: {e}\")\n",
        "\n",
        "def main():\n",
        "    audio_path = '/content/output_audio.mp3'\n",
        "    transcript_path = \"transcript_without_timestamps.txt\"\n",
        "    translated_text_path = \"/content/india_translated.txt\"\n",
        "    target_language = input(\"Enter the target language code (e.g., 'fr' for French, 'es' for Spanish): \")\n",
        "    input_video_path = \"/content/video_without_audio.mp4\"\n",
        "    output_video_path = \"/content/output_video_with_audio.mp4\"\n",
        "    output_audio_path = \"temp_out.mp3\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    transcript = transcribe_audio(audio_path, transcript_path)\n",
        "\n",
        "    if transcript:\n",
        "        try:\n",
        "            with open(transcript, 'r', encoding='utf-8') as file:\n",
        "                paragraph = file.read()\n",
        "            print(\"Original Text:\")\n",
        "            print(paragraph)\n",
        "\n",
        "            translated_paragraph = translate_paragraph(paragraph, target_language)\n",
        "            if translated_paragraph:\n",
        "                print(\"\\nTranslated Text:\")\n",
        "                print(translated_paragraph)\n",
        "\n",
        "                save_to_file(translated_paragraph, translated_text_path)\n",
        "\n",
        "                tts_audio = text_to_speech(translated_paragraph, output_audio_path, target_language)\n",
        "\n",
        "                if os.path.exists(input_video_path) and os.path.exists(tts_audio):\n",
        "                    merge_audio_with_video(input_video_path, tts_audio, output_video_path)\n",
        "                else:\n",
        "                    print(\"Input video or audio file not found.\")\n",
        "            else:\n",
        "                print(\"Translation failed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in processing the transcript: {e}\")\n",
        "    else:\n",
        "        print(\"Text extraction failed.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Execution time:\", end_time - start_time, \"seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fb0kdGqUUlX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12db83a6c01c4a15aeff1759460e2c81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1437b57d34fa4e6ab619ecc082cd7b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a105947868bd4ab4b15a0d4f3b1c25a3",
              "IPY_MODEL_456ccd933ce94b37890258110f3ef442",
              "IPY_MODEL_f0cabed56fd4412d972be4dfce0b0fc5"
            ],
            "layout": "IPY_MODEL_371e619c9df045ba87fdc67c981ee0ec"
          }
        },
        "186e950c40ef42099a9bdd9486e15ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20d0ac7955884249bad19d846eac3b34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "253e299aaa754e9eb64fb7362564ce0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c97788bdd143339d798980b0cbf438",
            "placeholder": "​",
            "style": "IPY_MODEL_afb809071827447cb5399c4b580aa2f0",
            "value": "100%"
          }
        },
        "371e619c9df045ba87fdc67c981ee0ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "456ccd933ce94b37890258110f3ef442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20d0ac7955884249bad19d846eac3b34",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186e950c40ef42099a9bdd9486e15ed9",
            "value": 1
          }
        },
        "60eed80653d644de9a36d65e38591681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "689eabe7a7104a62b642b52fd09f5d56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74dccebad14d4a40b133409f365c1b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "778d1cc3cc38441a8352fe1e8b852b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78608281a92342a1a607f5878175741b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c97788bdd143339d798980b0cbf438": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d71c118eb4a48cd9656fe4dea89e8d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12db83a6c01c4a15aeff1759460e2c81",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74dccebad14d4a40b133409f365c1b94",
            "value": 1
          }
        },
        "a105947868bd4ab4b15a0d4f3b1c25a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778d1cc3cc38441a8352fe1e8b852b7c",
            "placeholder": "​",
            "style": "IPY_MODEL_be1989a3938e4f27a0e03d7d6421ce97",
            "value": "100%"
          }
        },
        "a78d5af25af344bf8168ec83e7832415": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb809071827447cb5399c4b580aa2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba2efa5cf5a241fbad1cb2e385ef2149": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_253e299aaa754e9eb64fb7362564ce0e",
              "IPY_MODEL_8d71c118eb4a48cd9656fe4dea89e8d8",
              "IPY_MODEL_c66899e3f6214b8f9b9d76c49c315cd9"
            ],
            "layout": "IPY_MODEL_78608281a92342a1a607f5878175741b"
          }
        },
        "ba3c59096b60489cb4765640dcba876a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be1989a3938e4f27a0e03d7d6421ce97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c66899e3f6214b8f9b9d76c49c315cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689eabe7a7104a62b642b52fd09f5d56",
            "placeholder": "​",
            "style": "IPY_MODEL_ba3c59096b60489cb4765640dcba876a",
            "value": " 1/1 [00:00&lt;00:00,  3.91it/s]"
          }
        },
        "f0cabed56fd4412d972be4dfce0b0fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78d5af25af344bf8168ec83e7832415",
            "placeholder": "​",
            "style": "IPY_MODEL_60eed80653d644de9a36d65e38591681",
            "value": " 1/1 [00:00&lt;00:00, 51.43it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}